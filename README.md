# Alpaca Lora 4bit, long-range and other experiments

Forked from: https://github.com/johnsmith0031/alpaca_lora_4bit.git

LLaMA is pretrained using rotary embeddings spanning 2048 tokens. As a result it becomes useless for generation on longer input sequences.

With quantized weights and memory-efficient attention it's feasible to run inference on much longer sequences, though. This project is to test if a low-rank adapter can make up for LLaMA's lack of training on longer sequences.


## Results
~Nothing yet


# Update Logs

